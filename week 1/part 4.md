# HW 1, Part 4
**4.1, Huber Function**
1. Let $f(\lambda) = \lambda + \frac{x^2}{\lambda + d}$, so $$f'(\lambda) = 1 - \frac{x^2}{(\lambda + d)^2}.$$ If $|x|\le d$ then $\frac{x^2}{(\lambda+d)^2}\le 1$ and $f$ is monotonically increasing on $[0,\infty)$. So the minimum occurs when $\lambda = 0$, which gives $B(x, d) = \frac{x^2}{d}$. Otherwise, the minimum occurs when $f'(\lambda) = 0$, which directly solving gives $\lambda = |x| - d$, and $B(x,d) = 2|x|-d$.
2. Yes, $B$ is convex in $x$. We have $\frac{\partial^2}{\partial x^2}B = 2$ for $|x|\le d$ and $0$ for $|x|>d$, which is nonnegative, enough to prove convexity.
3. We have $$\nabla B = \begin{cases}(\frac{2x}{d},-\frac{x^2}{d})&|x|\le d\\(\frac{2x}{|x|},-1)&|x|>d.\end{cases}$$
4. When minimizing you go in the opposite direction of the gradient. As $d$ is a constant, this results in a pull of $-\frac{2x}{\max(|x|, d)}$. As $d$ decreases, $x$ will be pulled more strongly towards $0$.

**4.2, An application of using Huber loss to solve dual problem.**
Consider the optimization problem
$$\begin{align*}&\min_x\left[ f(x):=\sum_{i=1}^N\frac12d_ix_i^2+r_ix_i\right]\\&\text{s.t.}\quad a^Tx = 1, \quad x_i\in[-1, 1]\ \ \text{for}\ \ i=1,2,\dots,n.\end{align*}$$
1. If $||a||_1\le 1$ then we only get $a^Tx=1$ for all $|x_i|\ge 1$, so it's not strictly feasible. Now, if $||a||_1 > 1$, we can use a greedy algorithm to pick the $x$'s and get a strictly feasible solution.
2. Note: Problem 4.2.4 switches the standard convention of $\mu$ and $\lambda$ so I have reflected that here. The Lagrangian is $$L(x, \mu, \lambda) =  \sum_{i=1}^N\left(\frac12d_ix_i^2+r_ix_i\right) + \mu(a^Tx-1) + \sum_{i=1}^N\frac12\lambda_i\left(x_i^2-1\right)$$ Taking a gradient with respect to $x$ and setting it to zero gives $$x_i^* = -\frac{r_i+\mu a_i}{d_i+\lambda_i}.$$ Plugging back in gives the dual problem to maximize $$q(\mu, \lambda) = -\mu -\frac12\sum_{i=1}^N\frac{(r_i+\mu a_i)^2}{d_i+\lambda_i}.$$subject to $\lambda_i\ge 0$.
4. According to the KKT conditions, an optimal solution $x^*$ must satisfy:
	- **Stationarity**: $d_ix_i^* + r_i + \mu a_i + \lambda_ix_i^* = 0$
	- **Primal Feasibility**: $a^Tx^* - 1  =0$, and $(x_i^*)^2-1\le 0$ for $1\le i\le N$.
	- **Dual Feasibility:** $\lambda_i\ge 0$.
	- **Complementary Slackness:** $\sum_{i=1}^N\lambda_i((x_i^*)^2-1) = 0$.
The first and third KKT conditions were how we obtained the dual problem, and primal feasibility is basically Slater's condition and shown in 4.2.1 above. The final condition, complementary slackness, finishes characterizing the optimal solution, as Lagrange multipliers could be used to find $\lambda$ given $\mu$, and then you just need to maximize based on $\mu$.
4. Our problem is equivalent to $$\min_{\mu}\mu + \sum_{i=1}^{N}\frac{1}{2}\min_{\lambda_{i}}\frac{(r_i+\mu a_i)^2}{d_i+\lambda_i}$$From the slack condition, we have the following two cases for that inner minimum:$$\lambda_i = 0\implies \min_{\lambda_i}\frac{(r_i+\mu a_i)^2}{d_i+\lambda_i} = \frac{(r_i+\mu a_i)^2}{d_i},$$or$$\lambda_i\ne 0\implies |x_i^*| = 1\Longleftrightarrow \lambda_i = |r+\mu a_i|-d_i$$which gives a minimum of$$\min_{\lambda_i}\frac{(r_i+\mu a_i)^2}{d_i+\lambda_i} = |r_i+\mu a_i|<\frac{(r_i+\mu a_i)^2}{d_i}\qquad \text{if }|r_i+\mu a_i| > d_i.$$The minimum always occurs in the same place as the Huber function, even if the minimum isn't exactly the same, so the problem is equivalent to finding$$\min_\mu\mu + \frac12\sum_{i=1}^NB(r_i+\mu a_i,d_i).$$
5. We have$$-\frac12\cdot\frac{\partial B(r_i+\mu a_i, d_i)}{\partial \mu} = a_i\cdot\begin{cases}(r_i+\mu a_i)/d_i&|r_i+\mu a_i|\le d_i\\\text{sgn}(r_i+\mu a_i)&|r_i+\mu a_i|> d_i.\end{cases}$$where $\text{sgn}(x) = |x|/x$ is the sign of $x$. The sum of all of these needs to equal $1$. We cut the real line at the $4N$ points where $|r_i+\mu a_i| = d_i$ or $|r_i+\mu a_i|=0$. It takes $O(N)$ time to find the cuts, but we need to sort them so we end up with $O(N\log N)$ time complexity here. Now, we'll have a bunch of intervals: $I_1, I_2, I_3, \dots I_{4N+1}$, some of which may be empty. For each of these intervals $I_k$, we create latent variables$$z_i=\begin{cases}1&|r_i+\mu a_i|\le d_i\\0&|r_i+\mu a_i|>d_i\end{cases},$$$$s_i=\text{sgn}(r_i+\mu a_i),$$The optimal $\mu$ that falls in this interval would be$$\mu_k = \frac{1-\sum_{i=1}^N[z_ia_ir_i/d_i+(1-z_i)a_is_i]}{\sum_{i=1}^Nz_ia_i^2/d_i}.$$It would be a waste of time to constantly evaluate those sums (not to mention the latent variables), so we keep track of the numerator and denominator separately. As we loop through the cutting points, we only need to change the terms within each sum associated with that particular point, a constant time operation. So we can compute all the $\mu_k$'s in linear time. We ignore any $\mu_k\not\in I_k$ and sort the rest. Worst case this is $O(N\log N)$. Finally, we need to find the best $\mu$ from these. It takes $O(N)$ time to plug $\mu$ into the minimizer function, but luckily the function is convex. We can use a binary search on our sorted list of $\mu_k$'s in $O(N\log N)$ time. The overall time complexity is $O(N\log N)$.
6. Once you have $\mu$, you plug it into each of the $i$ Huber functions. It takes constant time to find the best $\lambda_i$ for each one. Once $\mu$ and $\lambda$ are determined, we can use the formula from above:$$x_i^* = -\frac{r_i+\mu a_i}{d_i+\lambda_i}.$$